 \section{Overview}
\label{sec:overview}
Here we will keep a log of updates
\subsection{First Mail highlights}
 \begin{figure*}[h]
 \begin{center}
  \begin{overpic}
  [trim=0cm 0cm 0cm 0cm,clip,width=\linewidth]{figures/original_ae.png}

  \end{overpic}
      \caption{First sketch of the network.}
    \label{fig:sketch}
\end{center}
\end{figure*}
The main idea is to connect the isospectralization idea to an autoencoder obtaining a stable toll for generating shapes from the spectrum. Given a shape S we can compute its spectrum as $e = spec(S)$. Now suppose we have an autoencoder (with $E$ (encoder) and $D$ (decoder)) that it is trained to generate shapes. 
$v = E(S)$  is a latent vector that represents the shape $S$ in a latent space.
$D(E(S)) = D(v)$ is the shape generated by the decoder that should be as equal as possible to $S$.
The space of spectra is a latent space and the idea is to consider a couple of nets $N1$ and $N2$ that connect the $2$ latent spaces.
The scheme can be depicted as in the Figure \ref{fig:sketch}: in this setting, we could optimize w.r.t. different losses:
\begin{itemize}
    \item $L_{ae} = || D(E(S)) -S ||$ for the autoencoder.
    \item $L_{N1} = || N1(e) - v ||$ for the connection between the latent spaces in the first direction (from $e$ to $v$)
    \item $L_{N2} = || N2(v) - e ||$ for the connection between the latent spaces in the second direction (from $v$ to $e$)
    \item $L_{con} = || N1(N2(Id)) - Id ||$ consistency (also the inverse can be considered)
    \red{\item A good alternative for $D$ could be $SPEC(D)$: $||SPEC(D(v)) - e ||$. This requires being able to back-propagate on eigenvalues, which we cannot do in reasonable time for now. 
    \item If the autoencoder is trained \textit{before} $N1$ and $N2$, a good alternative for $N2$ could be $|| N2(v) - SPEC(D) || $ with $SPEC(D)$ given.}
\end{itemize}
In this way we could obtain a tool for isopsectralization, for instance, $D(N1)$ is a generator of shapes from the spectrum. 
Through $N1$ and $N2$ we can optimize the value of $v$ in the latent space of the autoencoder to find a shape that as the desired spectrum $N2(v)$.



\subsection{Week 23-30 September}
 \begin{figure*}[h]
 \begin{center}
  \begin{overpic}
  [trim=0cm 0cm 0cm 0cm,clip,width=\linewidth]{figures/N1N2.pdf}

  \end{overpic}
      \caption{Implementation of the network.}
    \label{fig:sketch}
\end{center}
\end{figure*}
In this week we have implemented a first version of the network. From some experiments, it looks that $N1$ and $N2$ could not be too much simple (e.g. single layer, 150 neurons with tanh activation ends to collapse). The dataset considered is the CoMa, without the disconnected components of the eyes (i.e. 3931 vertices). At the moment I am using the loss:
\begin{equation}
    L = L_{ae} + L_{N1} + L_{N2}
\end{equation}
and the following hyper-parameters:
\begin{itemize}
    \item Latent Space: 30 units
    \item Learning rate: $1e-4$ for encoder-decoder; $4e-4$ or $8e-4$ for $N1$ and $N2$
    \item Batch Size: $512$
    \item Epochs: $1000$
    \item Eigenvalues: the first $30$
\end{itemize}
While $N1$ and $N2$ need some stabilization, you can see some results for $N2$ in Figure \ref{fig:resN2}.\\
Points to discuss:
\begin{itemize}
    \item Should $N1$ and $N2$ be trained together to the encoder, or separately? \red{Keep experimenting with both set-up}
    \item Should we consider $e$ as the eingenvalues of the meshes outputted by the decoder, instead of the train dataset?
    \item Regularization of the loss (e.g. weighting the eigenvalues). \red{This can be done for $L_{N2}$: 
    \begin{equation}\label{eq:weighted-norm}
        || N2(v) - e ||_w = \sum_i \frac{(N2(v)_i - e_i)^2}{e_i^2} = ||(N2(v) - e)/e||
    \end{equation} 
    }
    \item Exploit eigenvalues hierarchy to build the networks
\end{itemize}

 \begin{figure*}[h]
 \begin{center}
  \begin{overpic}
  [trim=0cm 0cm 0cm 0cm,clip,width=\linewidth]{figures/N2_loss_compressed.pdf}
  \put(8, 32){\footnotesize{Train Set}}
    \put(8,21){\footnotesize{Test Set}}
    \put(15,-1){\footnotesize{v}}
    \put(50,-1){N2(v)}
    \put(85,-1){e}
  \end{overpic}
      \caption{An example of result of the training $N2$ and encoder together. The scatters show \textbf{only} over the training set. \red{Could you add the PCA projections colored by expressions?}}
    \label{fig:resN2}
\end{center}
\end{figure*}

\subsection{Considerations and To-do}

\begin{itemize}
\item $N1$ appears to be very unstable. The reason could be that $N1$ is \textit{isospectralization} with an additional issue: in isospectralization we deform an initial shape in order to obtain another shape with a given spectrum, here we do not have any initial shape/starting point, so we are not able to disambiguate between isometries. In practice, with $L_{N1}$ we may be asking $N1$ to associate the same $e$ to different $v$ far apart in the latent space, i.e. not to be a function, causing numerical instability.\newline
An alternative to do isospectralization w.r.t. some initial shape $S_{in}$ could be, given $N2$ and the autoencoder, to solve the optimization problem over a deformation field in the latent space:
    \begin{equation}\label{eq:optimization}
        \min_{dv} || N2( E(S_{in}) + dv ) -  \lambda ||_w
    \end{equation}
where $\lambda$ is some target spectrum.
This is the initial idea of doing isospec. with a parametric model.
\item How much are $SPEC(S)$, $SPEC(D(v))$ and $N2(v)$ close right now in the train set and in the test set? (The distance between $2$ sets of eigenvalues can be quantified with \autoref{eq:weighted-norm})
\item \textbf{Deformation transfer}: what happens in the space of eigenvalues in a linear interpolation between two shapes in the $v$ space?
\item Consider replacing the eigenvalues of the train dataset with eigenvalues computed at higher order of FEM, maybe we are introducing extra noise for no reason
\item The dataset of triangles is probably not balanced, create another one

\end{itemize}


\subsection{Week 1-7 October}

\textbf{Training AE, N1 and N2 jointly:} I have thought that to stabilize N1 I could try to over-fit it to a subset of data. So I have reduced the dataset to $371$ heads by sampling the original $20$K. This change made $N1$ work; however, in this model, $N2$ is not working, maybe because I have to balance better the training. I have uploaded in the folder \emph{docs} the file \emph{N1ex.pdf}, where we have:
\begin{itemize}
    \item The latent space of train data, the projection of the test data into it, and the map performed by N1 starting from the related eigenvalues
    \item Some experiments of reconstruction with:
    \begin{itemize}
        \item the position of the candidate in the latent space using the encoder $E(S)$ , and the estimation obtained by $N1(spec(S))$ 
        \item the input shape, its autoencoder reconstruction $D(E(S))$ and $N1$ and decoder reconstruction $D(N1(spec(S)))$
        \item eigenvalues curves, and their differences through the weighted norm \ref{eq:weighted-norm}, with the sum value in the legend
        \item for the test-set examples, I have also provided the nearest neighbor in the latent space, among the test set examples. This is to check if the network has memorized some locations in the latent space (i.e. it maps all in the same already-seen heads), or it is generalizing
    \end{itemize}
\end{itemize}

\textbf{Training the networks disjointly:} I have performed some experiments of interpolation in the latent space using $N2$. I have uploaded in the folder \emph{docs} the file \emph{N2interp.pdf}, where we have:
\begin{itemize}
\item The latent space representation with the proposed interpolation
\item The input heads, their reconstruction $D(E(S))$ and their spectrum
\item The head generated by the interpolation, their spectrum, and the difference of the $N2(v)$ mapping between $spec(S)$ and $spec(D(E(S))$ with the through the weighted norm \ref{eq:weighted-norm}.
\end{itemize}
Black dots are the test set; in my experiments the fourth is between a shape of the train and one of the test, and the last one is with two shapes from the test.


\red{\begin{itemize}
    \item Interpolattion: Is the resulting path in the eigenvalues space linear as the one in the AE latent space? What if we do the linear interpolation in the eigenvalues space and then use $N1$ to see what happen in the latent space and to the shape $S$? Try to: \newline
    - Plot also the path in the $N2(v)$ space with PCA
    \item Can we give in input $N1$ totally arbitrary eigenvalues?
    \item Show a video
    Try to: \newline
    - Remesh shape before computing eigenvalues \newline
    - scale eigenvalues and see if the shape scale accordingly
    \item Can we add the second SPEC every once in a while?
    \item To stabilize the optimization \autoref{eq:optimization}, we could add a regularizer on $||dv||$, to avoid getting away too much from the initial shape
    \item use NN with training set as baseline for the maps, showing nearest neighbor in the latent and eigen spaces
    \item do backpropagation on $v$ space with N1
\end{itemize}}

\subsection{Week 8-15 October}
Experiments:
\begin{itemize}
    \item isospectral comparison at two different resolutions in Figures \ref{fig:1k} and \ref{fig:500}
    \item quantitative evaluations in table \ref{tab:quantitative}
    \item trying to fit eigenvalues by backpropagating in the latent space (using N2)
    with MSE loss:
    \begin{itemize}
    \item \url{https://www.youtube.com/watch?v=5G87BGLKP7k}
    \item \url{https://www.youtube.com/watch?v=XNsxZI2BDHM}
    \end{itemize}
    and with the regularized loss in Equation \ref{eq:weighted-norm}:
    \begin{itemize}
    \item \url{https://www.youtube.com/watch?v=g-fXPPIdaf8}
    \item \url{https://www.youtube.com/watch?v=YOGe_8JNLCw}
    \end{itemize}
    \item trying to fit latent space by backpropagating to the eigenvalues (using N1)
    \begin{itemize}
        \item \url{https://youtu.be/IWt1oJm54XA}

    \end{itemize}
    
\end{itemize}
 \begin{figure}[h]
 \begin{center}
  \begin{overpic}
  [trim=0cm 0cm 0cm 0cm,clip,width=\linewidth]{figures/1k.png}

  \end{overpic}
      \caption{isospec compare: 1k vertices}
    \label{fig:1k}
\end{center}
\end{figure}

 \begin{figure}[h]
 \begin{center}
  \begin{overpic}
  [trim=0cm 0cm 0cm 0cm,clip,width=\linewidth]{figures/500.png}

  \end{overpic}
      \caption{isospec compare: 500 vertices}
    \label{fig:500}
\end{center}
\end{figure}

\begin{table}[]
\begin{tabular}{lllll}
              & Train  & Test   &  &  \\
$D(E(S))$       & 0.0011 & 0.0013 &  &  \\
$D(N1(spec(S))$ & 0.0029 & 0.0033 &  &  \\
PCA           & 0.0005 & 0.0008 &  &  \\ \cline{1-3}
$N2(E(S))$      & 0.0096 & 0.0154 &  &  \\
$NN_{train}$ & 0      & 0.0047 &  & 
\end{tabular}
\caption{quantitative comparison}
\label{tab:quantitative}
\end{table}