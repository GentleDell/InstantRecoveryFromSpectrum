 \section{Overview}
\label{sec:overview}
Here we will keep a log of updates
\subsection{First Mail highlights}
 \begin{figure*}[h]
 \begin{center}
  \begin{overpic}
  [trim=0cm 0cm 0cm 0cm,clip,width=\linewidth]{figures/original_ae.png}

  \end{overpic}
      \caption{First sketch of the network.}
    \label{fig:sketch}
\end{center}
\end{figure*}
The main idea is to connect the isospectralization idea to an autoencoder obtaining a stable toll for generating shapes from the spectrum. Given a shape S we can compute its spectrum as $e = spec(S)$. Now suppose we have an autoencoder (with $E$ (encoder) and $D$ (decoder)) that it is trained to generate shapes. 
$v = E(S)$  is a latent vector that represents the shape $S$ in a latent space.
$D(E(S)) = D(v)$ is the shape generated by the decoder that should be as equal as possible to $S$.
The space of spectra is a latent space and the idea is to consider a couple of nets $N1$ and $N2$ that connect the $2$ latent spaces.
The scheme can be depicted as in the Figure \ref{fig:sketch}: in this setting, we could optimize w.r.t. different losses:
\begin{itemize}
    \item $|| D(E(S)) -S ||$ for the autoencoder.
\item $|| N1(e) - v ||$ the connection between the latent spaces in the first direction
\item $|| N2(v) - e ||$ the connection between the latent spaces in the second direction
\item $|| N1(N2(Id)) - Id ||$ consistency (also the inverse can be considered)
a good alternative for $N2$ could be $SPEC(D)$ i.e. $||SPEC(D(v)) - e ||$.

\end{itemize}
In this way we could obtain a tool for isopsectralization, for instance, $D(N1)$ is a generator of shapes from the spectrum. 
Through $N1$ and $N2$ we can optimize the value of $v$ in the latent space of the autoencoder to find a shape that as the desired spectrum $N2(v)$.
